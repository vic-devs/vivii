{
 "cells": [
  {
   "cell_type": "code",
   "id": "08b3d80c-1570-4865-b936-c832b47ba214",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import arff\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from surprise import Dataset, Reader, KNNWithMeans, SVD, accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "# Define interaction values with weights for various user actions.\n",
    "# These weights determine the relative importance of each interaction type.\n",
    "interaction_values = {\n",
    "    b'browse': 3,\n",
    "    b'view': 4,  # Lower priority for items the user simply viewed\n",
    "    b'add_to_cart': 4,\n",
    "    b'remove_from_cart': 0,\n",
    "    b'add_to_favorite': 5,\n",
    "    b'remove_from_favorite': 0,\n",
    "    b'completed_checkout': 7  # Highest weight for completed purchases\n",
    "}\n",
    "\n",
    "# Function to print debugging information about a DataFrame\n",
    "def debug_data(data, name=\"DataFrame\"):\n",
    "    print(f\"--- Debugging {name} ---\")\n",
    "    print(\"NaN Values:\\n\", data.isna().sum())  # Check for missing values\n",
    "    print(\"Sample Data:\\n\", data.head())       # Print a sample of the data\n",
    "    print(\"Variance of interaction_value:\\n\", data['interaction_value'].var())  # Show variance\n",
    "\n",
    "# Load and preprocess data from multiple files\n",
    "def load_data(user_activity_path, orders_path, order_items_path, products_path, users_path):\n",
    "    scaler = MinMaxScaler()  # Initialize scaler for normalization\n",
    "    \n",
    "    # Load datasets\n",
    "    data, meta = arff.loadarff(user_activity_path)  # Load ARFF data\n",
    "    user_activity_df = pd.DataFrame(data)           # Convert to DataFrame\n",
    "    orders_df = pd.read_csv(orders_path)            # Load orders\n",
    "    order_items_df = pd.read_csv(order_items_path)  # Load order items\n",
    "    products_df = pd.read_csv(products_path)        # Load product details\n",
    "    user_data = pd.read_csv(users_path)             # Load user data with top categories\n",
    "\n",
    "    # Rename product and order IDs for consistency\n",
    "    products_df.rename(columns={'id': 'product_id'}, inplace=True)\n",
    "    orders_df.rename(columns={'id': 'order_id'}, inplace=True)\n",
    "    \n",
    "    # Convert timestamps to datetime format and calculate time decay\n",
    "    user_activity_df['timestamp'] = pd.to_datetime(user_activity_df['activity_timestamp'])\n",
    "    max_timestamp = user_activity_df['timestamp'].max()\n",
    "    # Calculate time decay as days since last activity\n",
    "    user_activity_df['time_decay'] = (max_timestamp - user_activity_df['timestamp']).dt.total_seconds() / (24 * 60 * 60)\n",
    "    user_activity_df['time_decay'] = np.exp(-user_activity_df['time_decay'] / 30)  # Apply decay factor with 30-day half-life\n",
    "    \n",
    "    # Map each interaction type to a predefined weight and apply time decay\n",
    "    user_activity_df['interaction_value'] = user_activity_df['activity_type'].map(interaction_values)\n",
    "    user_activity_df['interaction_value'] *= user_activity_df['time_decay']\n",
    "    \n",
    "    # Adjust for session length if `activity_duration` is available\n",
    "    if 'activity_duration' in user_activity_df.columns:\n",
    "        user_activity_df['session_weight'] = scaler.fit_transform(user_activity_df[['activity_duration']].fillna(0))\n",
    "    else:\n",
    "        user_activity_df['session_weight'] = 0  # Set zero weight if session length is missing\n",
    "    \n",
    "    # Multiply interaction value by session weight\n",
    "    user_activity_df['interaction_value'] *= (1 + user_activity_df['session_weight'])\n",
    "    \n",
    "    # Fill missing values in `interaction_value` with the mean value\n",
    "    user_activity_df['interaction_value'].fillna(user_activity_df['interaction_value'].mean(), inplace=True)\n",
    "    \n",
    "    # Process completed orders with high priority\n",
    "    orders_with_items = pd.merge(order_items_df, orders_df, on='order_id')\n",
    "    orders_with_items['interaction_value'] = interaction_values[b'completed_checkout']\n",
    "    \n",
    "    # Combine all interactions for training the model\n",
    "    all_interactions = pd.concat([\n",
    "        user_activity_df[['user_id', 'product_id', 'interaction_value']],\n",
    "        orders_with_items[['user_id', 'product_id', 'interaction_value']]\n",
    "    ])\n",
    "    \n",
    "    # Normalize interaction values to fit the model's input requirements\n",
    "    all_interactions['interaction_value_normalized'] = scaler.fit_transform(all_interactions[['interaction_value']])\n",
    "    \n",
    "    # Print debugging information\n",
    "    debug_data(all_interactions, \"Combined Interactions\")\n",
    "    \n",
    "    return all_interactions, products_df, user_data\n",
    "\n",
    "# Load and preprocess data from specified files\n",
    "all_interactions, products_df, user_data = load_data(\n",
    "    'cleaned__user_activity1.arff', 'orders.csv', 'order_items.csv', 'products.csv', 'users.csv'\n",
    ")\n",
    "\n",
    "# Train KNN and SVD models, and calculate RMSE for both training and test sets\n",
    "def train_models(interactions):\n",
    "    # Prepare data for training\n",
    "    reader = Reader(rating_scale=(0, 1))\n",
    "    data = Dataset.load_from_df(interactions[['user_id', 'product_id', 'interaction_value_normalized']], reader)\n",
    "    trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train KNN model with specified parameters\n",
    "    sim_options = {'name': 'pearson_baseline', 'user_based': True, 'min_support': 3}\n",
    "    knn_model = KNNWithMeans(k=40, min_k=2, sim_options=sim_options)\n",
    "    knn_model.fit(trainset)\n",
    "    \n",
    "    # Evaluate KNN model on training and test sets\n",
    "    knn_train_predictions = knn_model.test(trainset.build_testset())\n",
    "    knn_train_rmse = accuracy.rmse(knn_train_predictions, verbose=False)\n",
    "    knn_test_predictions = knn_model.test(testset)\n",
    "    knn_test_rmse = accuracy.rmse(knn_test_predictions, verbose=False)\n",
    "    print(f\"KNN Training RMSE: {knn_train_rmse:.4f}, KNN Testing RMSE: {knn_test_rmse:.4f}\")\n",
    "    \n",
    "    # Train SVD model with matrix factorization\n",
    "    svd_model = SVD(n_factors=100, n_epochs=20, lr_all=0.005, reg_all=0.02)\n",
    "    svd_model.fit(trainset)\n",
    "    \n",
    "    # Evaluate SVD model on training and test sets\n",
    "    svd_train_predictions = svd_model.test(trainset.build_testset())\n",
    "    svd_train_rmse = accuracy.rmse(svd_train_predictions, verbose=False)\n",
    "    svd_test_predictions = svd_model.test(testset)\n",
    "    svd_test_rmse = accuracy.rmse(svd_test_predictions, verbose=False)\n",
    "    print(f\"SVD Training RMSE: {svd_train_rmse:.4f}, SVD Testing RMSE: {svd_test_rmse:.4f}\")\n",
    "    \n",
    "    return knn_model, svd_model\n",
    "\n",
    "# Run the training function and print RMSE results\n",
    "knn_model, svd_model = train_models(all_interactions)\n",
    "\n",
    "# Hybrid recommendation function to generate personalized recommendations for a user\n",
    "def hybrid_recommend(user_id, n=5, weights=(0.6, 0.4)):\n",
    "    # Retrieve items that the user has completed checkout for (purchased)\n",
    "    purchased_items = set(all_interactions[\n",
    "        (all_interactions['user_id'] == user_id) & \n",
    "        (all_interactions['interaction_value'] == interaction_values[b'completed_checkout'])\n",
    "    ]['product_id'])\n",
    "    \n",
    "    # Identify items to predict by excluding already purchased items\n",
    "    all_items = set(all_interactions['product_id'])\n",
    "    items_to_predict = list(all_items - purchased_items)\n",
    "\n",
    "    # Retrieve user's top preferred categories from `user_data`\n",
    "    user_top_categories = user_data.loc[user_data['id'] == user_id, \n",
    "                                        ['top_category1', 'top_category2', 'top_category3']].values.flatten()\n",
    "    \n",
    "    predictions = []\n",
    "    for item_id in items_to_predict:\n",
    "        # Check that the item category exists in `products_df`\n",
    "        item_category_row = products_df.loc[products_df['product_id'] == item_id, 'category_id']\n",
    "        if item_category_row.empty:\n",
    "            continue  # Skip items with missing category information\n",
    "        \n",
    "        item_category = item_category_row.values[0]\n",
    "        \n",
    "        # Apply category boost if item belongs to user's top categories\n",
    "        category_boost = 1.2 if item_category in user_top_categories else 1.0\n",
    "        \n",
    "        # Get predictions from KNN and SVD models, then combine with weights and boost\n",
    "        knn_pred = knn_model.predict(user_id, item_id).est\n",
    "        svd_pred = svd_model.predict(user_id, item_id).est\n",
    "        hybrid_score = (weights[0] * knn_pred + weights[1] * svd_pred) * category_boost\n",
    "        predictions.append((item_id, hybrid_score))\n",
    "    \n",
    "    # Select top N recommendations and merge with product details\n",
    "    top_n = sorted(predictions, key=lambda x: x[1], reverse=True)[:n]\n",
    "    recommendations = pd.DataFrame(top_n, columns=['product_id', 'predicted_score'])\n",
    "    recommendations = pd.merge(recommendations, products_df[['product_id', 'name', 'price', 'category_id']], on='product_id', how='left')\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Generate recommendations for a specific user\n",
    "test_user_id = 5\n",
    "recommendations = hybrid_recommend(test_user_id, n=10)\n",
    "print(f\"\\nTop 5 recommendations for user {test_user_id}:\")\n",
    "print(recommendations.to_string(index=False))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfa8cc4-59b2-435a-a024-731d8b7405f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e978018-ce0f-4840-9bc4-8d1c07601316",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
