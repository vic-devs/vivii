{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08b3d80c-1570-4865-b936-c832b47ba214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- cleaned__user_activity1.arff (User activity data)\n",
      "- orders.csv (Orders data)\n",
      "- order_items.csv (Order items data)\n",
      "- products.csv (Products data)\n",
      "- users.csv (User data)\n",
      "--- Debugging Combined Interactions ---\n",
      "NaN Values:\n",
      " user_id                         0\n",
      "product_id                      0\n",
      "interaction_value               0\n",
      "interaction_value_normalized    0\n",
      "dtype: int64\n",
      "Sample Data:\n",
      "    user_id  product_id  interaction_value  interaction_value_normalized\n",
      "0      2.0   14.482558               3.15                      0.424179\n",
      "1      2.0   14.482558               3.10                      0.417446\n",
      "2      2.0   14.482558               3.05                      0.410713\n",
      "3      2.0   14.482558               3.10                      0.417446\n",
      "4      2.0   14.482558               3.10                      0.417446\n",
      "Variance of interaction_value:\n",
      " 2.6201010113807865\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "KNN Training RMSE: 0.1773, KNN Testing RMSE: 0.2229\n",
      "SVD Training RMSE: 0.1836, SVD Testing RMSE: 0.2129\n",
      "\n",
      "Top 5 recommendations for user 5:\n",
      " product_id  predicted_score                                   name  price  category_id\n",
      "       18.0         0.701170                           Dining Table 199.99            2\n",
      "       10.0         0.689696 Drop-shaped Breastplate With Silicone   54.00            6\n",
      "       37.0         0.676106  Martin Luther King, Jr.: A Biography   19.99            8\n",
      "       31.0         0.670273                             Silk Scarf  49.99            6\n",
      "       17.0         0.666221                                   Sofa 299.99            2\n",
      "       12.0         0.633833           Small Talk that Doesn't Suck  30.47            8\n",
      "       19.0         0.622397                           Office Chair  89.99            2\n",
      "       30.0         0.606811                         Leather Jacket 249.99            6\n",
      "       14.0         0.604040                                T-Shirt  15.99            1\n",
      "        4.0         0.592528          Kazeila Artificial Lemon Tree  79.00            3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import arff\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from surprise import Dataset, Reader, KNNWithMeans, SVD, accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "# Define interaction values with weights for various user actions.\n",
    "# These weights determine the relative importance of each interaction type.\n",
    "interaction_values = {\n",
    "    b'browse': 3,\n",
    "    b'view': 4,  # Lower priority for items the user simply viewed\n",
    "    b'add_to_cart': 4,\n",
    "    b'remove_from_cart': 0,\n",
    "    b'add_to_favorite': 5,\n",
    "    b'remove_from_favorite': 0,\n",
    "    b'completed_checkout': 7  # Highest weight for completed purchases\n",
    "}\n",
    "\n",
    "# Function to print debugging information about a DataFrame\n",
    "def debug_data(data, name=\"DataFrame\"):\n",
    "    print(f\"--- Debugging {name} ---\")\n",
    "    print(\"NaN Values:\\n\", data.isna().sum())  # Check for missing values\n",
    "    print(\"Sample Data:\\n\", data.head())       # Print a sample of the data\n",
    "    print(\"Variance of interaction_value:\\n\", data['interaction_value'].var())  # Show variance\n",
    "\n",
    "def load_data(user_activity_path, orders_path, order_items_path, products_path, users_path):\n",
    "    print(f\"- {user_activity_path} (User activity data)\")\n",
    "    print(f\"- {orders_path} (Orders data)\")\n",
    "    print(f\"- {order_items_path} (Order items data)\")\n",
    "    print(f\"- {products_path} (Products data)\")\n",
    "    print(f\"- {users_path} (User data)\")\n",
    "\n",
    "    scaler = MinMaxScaler()  # Initialize scaler for normalization\n",
    "    \n",
    "    # Load datasets\n",
    "    data, meta = arff.loadarff(user_activity_path)  # Load ARFF data\n",
    "    user_activity_df = pd.DataFrame(data)           # Convert to DataFrame\n",
    "    orders_df = pd.read_csv(orders_path)            # Load orders\n",
    "    order_items_df = pd.read_csv(order_items_path)  # Load order items\n",
    "    products_df = pd.read_csv(products_path)        # Load product details\n",
    "    user_data = pd.read_csv(users_path)             # Load user data with top categories\n",
    "\n",
    "    # Rename product and order IDs for consistency\n",
    "    products_df.rename(columns={'id': 'product_id'}, inplace=True)\n",
    "    orders_df.rename(columns={'id': 'order_id'}, inplace=True)\n",
    "    \n",
    "    # Convert timestamps to datetime format and calculate time decay\n",
    "    user_activity_df['timestamp'] = pd.to_datetime(user_activity_df['activity_timestamp'])\n",
    "    max_timestamp = user_activity_df['timestamp'].max()\n",
    "    # Calculate time decay as days since last activity\n",
    "    user_activity_df['time_decay'] = (max_timestamp - user_activity_df['timestamp']).dt.total_seconds() / (24 * 60 * 60)\n",
    "    user_activity_df['time_decay'] = np.exp(-user_activity_df['time_decay'] / 30)  # Apply decay factor with 30-day half-life\n",
    "    \n",
    "    # Map each interaction type to a predefined weight and apply time decay\n",
    "    user_activity_df['interaction_value'] = user_activity_df['activity_type'].map(interaction_values)\n",
    "    user_activity_df['interaction_value'] *= user_activity_df['time_decay']\n",
    "    \n",
    "    # Adjust for session length if `activity_duration` is available\n",
    "    if 'activity_duration' in user_activity_df.columns:\n",
    "        user_activity_df['session_weight'] = scaler.fit_transform(user_activity_df[['activity_duration']].fillna(0))\n",
    "    else:\n",
    "        user_activity_df['session_weight'] = 0  # Set zero weight if session length is missing\n",
    "    \n",
    "    # Multiply interaction value by session weight\n",
    "    user_activity_df['interaction_value'] *= (1 + user_activity_df['session_weight'])\n",
    "    \n",
    "    # Fill missing values in `interaction_value` with the mean value\n",
    "    user_activity_df['interaction_value'].fillna(user_activity_df['interaction_value'].mean(), inplace=True)\n",
    "    \n",
    "    # Process completed orders with high priority\n",
    "    orders_with_items = pd.merge(order_items_df, orders_df, on='order_id')\n",
    "    orders_with_items['interaction_value'] = interaction_values[b'completed_checkout']\n",
    "    \n",
    "    # Combine all interactions for training the model\n",
    "    all_interactions = pd.concat([\n",
    "        user_activity_df[['user_id', 'product_id', 'interaction_value']],\n",
    "        orders_with_items[['user_id', 'product_id', 'interaction_value']]\n",
    "    ])\n",
    "    \n",
    "    # Normalize interaction values to fit the model's input requirements\n",
    "    all_interactions['interaction_value_normalized'] = scaler.fit_transform(all_interactions[['interaction_value']])\n",
    "    \n",
    "    # Print debugging information\n",
    "    debug_data(all_interactions, \"Combined Interactions\")\n",
    "    \n",
    "    return all_interactions, products_df, user_data\n",
    "\n",
    "# Load and preprocess data from specified files\n",
    "all_interactions, products_df, user_data = load_data(\n",
    "    'cleaned__user_activity1.arff', 'orders.csv', 'order_items.csv', 'products.csv', 'users.csv'\n",
    ")\n",
    "\n",
    "# Train KNN and SVD models, and calculate RMSE for both training and test sets\n",
    "def train_models(interactions):\n",
    "    # Prepare data for training\n",
    "    reader = Reader(rating_scale=(0, 1))\n",
    "    data = Dataset.load_from_df(interactions[['user_id', 'product_id', 'interaction_value_normalized']], reader)\n",
    "    trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train KNN model with specified parameters\n",
    "    sim_options = {'name': 'pearson_baseline', 'user_based': True, 'min_support': 3}\n",
    "    knn_model = KNNWithMeans(k=40, min_k=2, sim_options=sim_options)\n",
    "    knn_model.fit(trainset)\n",
    "    \n",
    "    # Evaluate KNN model on training and test sets\n",
    "    knn_train_predictions = knn_model.test(trainset.build_testset())\n",
    "    knn_train_rmse = accuracy.rmse(knn_train_predictions, verbose=False)\n",
    "    knn_test_predictions = knn_model.test(testset)\n",
    "    knn_test_rmse = accuracy.rmse(knn_test_predictions, verbose=False)\n",
    "    print(f\"KNN Training RMSE: {knn_train_rmse:.4f}, KNN Testing RMSE: {knn_test_rmse:.4f}\")\n",
    "    \n",
    "    # Train SVD model with matrix factorization\n",
    "    svd_model = SVD(n_factors=100, n_epochs=20, lr_all=0.005, reg_all=0.02)\n",
    "    svd_model.fit(trainset)\n",
    "    \n",
    "    # Evaluate SVD model on training and test sets\n",
    "    svd_train_predictions = svd_model.test(trainset.build_testset())\n",
    "    svd_train_rmse = accuracy.rmse(svd_train_predictions, verbose=False)\n",
    "    svd_test_predictions = svd_model.test(testset)\n",
    "    svd_test_rmse = accuracy.rmse(svd_test_predictions, verbose=False)\n",
    "    print(f\"SVD Training RMSE: {svd_train_rmse:.4f}, SVD Testing RMSE: {svd_test_rmse:.4f}\")\n",
    "    \n",
    "    return knn_model, svd_model\n",
    "\n",
    "# Run the training function and print RMSE results\n",
    "knn_model, svd_model = train_models(all_interactions)\n",
    "\n",
    "# Hybrid recommendation function to generate personalized recommendations for a user\n",
    "def hybrid_recommend(user_id, n=5, weights=(0.6, 0.4)):\n",
    "    # Retrieve items that the user has completed checkout for (purchased)\n",
    "    purchased_items = set(all_interactions[\n",
    "        (all_interactions['user_id'] == user_id) & \n",
    "        (all_interactions['interaction_value'] == interaction_values[b'completed_checkout'])\n",
    "    ]['product_id'])\n",
    "    \n",
    "    # Identify items to predict by excluding already purchased items\n",
    "    all_items = set(all_interactions['product_id'])\n",
    "    items_to_predict = list(all_items - purchased_items)\n",
    "\n",
    "    # Retrieve user's top preferred categories from `user_data`\n",
    "    user_top_categories = user_data.loc[user_data['id'] == user_id, \n",
    "                                        ['top_category1', 'top_category2', 'top_category3']].values.flatten()\n",
    "    \n",
    "    predictions = []\n",
    "    for item_id in items_to_predict:\n",
    "        # Check that the item category exists in `products_df`\n",
    "        item_category_row = products_df.loc[products_df['product_id'] == item_id, 'category_id']\n",
    "        if item_category_row.empty:\n",
    "            continue  # Skip items with missing category information\n",
    "        \n",
    "        item_category = item_category_row.values[0]\n",
    "        \n",
    "        # Apply category boost if item belongs to user's top categories\n",
    "        category_boost = 1.2 if item_category in user_top_categories else 1.0\n",
    "        \n",
    "        # Get predictions from KNN and SVD models, then combine with weights and boost\n",
    "        knn_pred = knn_model.predict(user_id, item_id).est\n",
    "        svd_pred = svd_model.predict(user_id, item_id).est\n",
    "        hybrid_score = (weights[0] * knn_pred + weights[1] * svd_pred) * category_boost\n",
    "        predictions.append((item_id, hybrid_score))\n",
    "    \n",
    "    # Select top N recommendations and merge with product details\n",
    "    top_n = sorted(predictions, key=lambda x: x[1], reverse=True)[:n]\n",
    "    recommendations = pd.DataFrame(top_n, columns=['product_id', 'predicted_score'])\n",
    "    recommendations = pd.merge(recommendations, products_df[['product_id', 'name', 'price', 'category_id']], on='product_id', how='left')\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Generate recommendations for a specific user\n",
    "test_user_id = 5\n",
    "recommendations = hybrid_recommend(test_user_id, n=10)\n",
    "print(f\"\\nTop 5 recommendations for user {test_user_id}:\")\n",
    "print(recommendations.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e978018-ce0f-4840-9bc4-8d1c07601316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and saving the hybrid recommendation model...\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Hybrid model saved to hybrid_recommendation_model.pkl\n",
      "KNN Training RMSE: 0.1773, KNN Testing RMSE: 0.2225\n",
      "SVD Training RMSE: 0.1869, SVD Testing RMSE: 0.2018\n",
      "Hybrid model loaded from hybrid_recommendation_model.pkl\n",
      "\n",
      "Top 5 recommendations for user 5:\n",
      " product_id  predicted_score                                   name  price  category_id\n",
      "       10.0         0.681630 Drop-shaped Breastplate With Silicone   54.00            6\n",
      "       18.0         0.668374                           Dining Table 199.99            2\n",
      "       17.0         0.666437                                   Sofa 299.99            2\n",
      "       30.0         0.663263                         Leather Jacket 249.99            6\n",
      "       19.0         0.660229                           Office Chair  89.99            2\n",
      "       31.0         0.632300                             Silk Scarf  49.99            6\n",
      "       12.0         0.630761           Small Talk that Doesn't Suck  30.47            8\n",
      "       37.0         0.620583  Martin Luther King, Jr.: A Biography   19.99            8\n",
      "        4.0         0.598266          Kazeila Artificial Lemon Tree  79.00            3\n",
      "       14.0         0.585712                                T-Shirt  15.99            1\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Define train and save hybrid model, similar to your previous example\n",
    "def train_and_save_hybrid_model(trainset, svd_filename='svd_recommendation_model.pkl', knn_filename='knn_recommendation_model.pkl', hybrid_filename='hybrid_recommendation_model.pkl'):\n",
    "    # Train SVD and KNN models\n",
    "    svd_model = SVD(n_factors=50, n_epochs=20, lr_all=0.005, reg_all=0.02, random_state=42)\n",
    "    knn_model = KNNWithMeans(k=20, min_k=2, sim_options={'name': 'pearson_baseline', 'user_based': True, 'min_support': 2})\n",
    "    svd_model.fit(trainset)\n",
    "    knn_model.fit(trainset)\n",
    "    \n",
    "    # Save individual models for standalone testing or future use\n",
    "    with open(svd_filename, 'wb') as file:\n",
    "        pickle.dump(svd_model, file)\n",
    "    with open(knn_filename, 'wb') as file:\n",
    "        pickle.dump(knn_model, file)\n",
    "    \n",
    "    # Create a hybrid model dictionary\n",
    "    hybrid_model = {\n",
    "        'svd_model': svd_model,\n",
    "        'knn_model': knn_model\n",
    "    }\n",
    "    \n",
    "    # Save the hybrid model\n",
    "    with open(hybrid_filename, 'wb') as file:\n",
    "        pickle.dump(hybrid_model, file)\n",
    "    \n",
    "    print(f\"Hybrid model saved to {hybrid_filename}\")\n",
    "    return hybrid_model\n",
    "\n",
    "# Update train_models function to use train_and_save_hybrid_model\n",
    "def train_models(interactions):\n",
    "    reader = Reader(rating_scale=(0, 1))\n",
    "    data = Dataset.load_from_df(interactions[['user_id', 'product_id', 'interaction_value_normalized']], reader)\n",
    "    trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Use train_and_save_hybrid_model function to train and save models\n",
    "    hybrid_model = train_and_save_hybrid_model(trainset)\n",
    "    \n",
    "    # Calculate RMSE for both models\n",
    "    knn_train_predictions = hybrid_model['knn_model'].test(trainset.build_testset())\n",
    "    knn_test_predictions = hybrid_model['knn_model'].test(testset)\n",
    "    knn_train_rmse = accuracy.rmse(knn_train_predictions, verbose=False)\n",
    "    knn_test_rmse = accuracy.rmse(knn_test_predictions, verbose=False)\n",
    "    print(f\"KNN Training RMSE: {knn_train_rmse:.4f}, KNN Testing RMSE: {knn_test_rmse:.4f}\")\n",
    "    \n",
    "    svd_train_predictions = hybrid_model['svd_model'].test(trainset.build_testset())\n",
    "    svd_test_predictions = hybrid_model['svd_model'].test(testset)\n",
    "    svd_train_rmse = accuracy.rmse(svd_train_predictions, verbose=False)\n",
    "    svd_test_rmse = accuracy.rmse(svd_test_predictions, verbose=False)\n",
    "    print(f\"SVD Training RMSE: {svd_train_rmse:.4f}, SVD Testing RMSE: {svd_test_rmse:.4f}\")\n",
    "    \n",
    "    return hybrid_model\n",
    "\n",
    "# Load trained models and use for hybrid recommendations\n",
    "def load_hybrid_model(hybrid_filename='hybrid_recommendation_model.pkl'):\n",
    "    with open(hybrid_filename, 'rb') as file:\n",
    "        hybrid_model = pickle.load(file)\n",
    "    print(f\"Hybrid model loaded from {hybrid_filename}\")\n",
    "    return hybrid_model\n",
    "\n",
    "# Hybrid recommendation function for generating personalized recommendations\n",
    "def hybrid_recommend(user_id, hybrid_model, n=5, weights=(0.6, 0.4)):\n",
    "    purchased_items = set(all_interactions[\n",
    "        (all_interactions['user_id'] == user_id) & \n",
    "        (all_interactions['interaction_value'] == interaction_values[b'completed_checkout'])\n",
    "    ]['product_id'])\n",
    "    all_items = set(all_interactions['product_id'])\n",
    "    items_to_predict = list(all_items - purchased_items)\n",
    "    \n",
    "    user_top_categories = user_data.loc[user_data['id'] == user_id, \n",
    "                                        ['top_category1', 'top_category2', 'top_category3']].values.flatten()\n",
    "    \n",
    "    predictions = []\n",
    "    for item_id in items_to_predict:\n",
    "        item_category_row = products_df.loc[products_df['product_id'] == item_id, 'category_id']\n",
    "        if item_category_row.empty:\n",
    "            continue\n",
    "        \n",
    "        item_category = item_category_row.values[0]\n",
    "        category_boost = 1.2 if item_category in user_top_categories else 1.0\n",
    "        \n",
    "        knn_pred = hybrid_model['knn_model'].predict(user_id, item_id).est\n",
    "        svd_pred = hybrid_model['svd_model'].predict(user_id, item_id).est\n",
    "        hybrid_score = (weights[0] * knn_pred + weights[1] * svd_pred) * category_boost\n",
    "        predictions.append((item_id, hybrid_score))\n",
    "    \n",
    "    top_n = sorted(predictions, key=lambda x: x[1], reverse=True)[:n]\n",
    "    recommendations = pd.DataFrame(top_n, columns=['product_id', 'predicted_score'])\n",
    "    recommendations = pd.merge(recommendations, products_df[['product_id', 'name', 'price', 'category_id']], on='product_id', how='left')\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Train and save the hybrid model\n",
    "print(\"Training and saving the hybrid recommendation model...\")\n",
    "hybrid_model = train_models(all_interactions)\n",
    "\n",
    "# Load the model and generate recommendations for a specific user\n",
    "hybrid_model = load_hybrid_model()\n",
    "test_user_id = 5\n",
    "recommendations = hybrid_recommend(test_user_id, hybrid_model, n=10)\n",
    "print(f\"\\nTop 5 recommendations for user {test_user_id}:\")\n",
    "print(recommendations.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c705828-6368-43d5-abec-5085ea5f037b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
